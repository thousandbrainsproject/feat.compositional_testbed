{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tbp.monty.frameworks.utils.logging_utils import load_stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tbp.monty.frameworks.utils.plot_utils import plot_graph\n",
    "import colorsys\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23276d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = os.path.expanduser(\"~/tbp/results/monty/pretrained_models/\")\n",
    "pretrained_dict = pretrain_path + \"pretrained_ycb_v10/supervised_pre_training_objects_wo_logos/pretrained/\"\n",
    "\n",
    "log_path = os.path.expanduser(\"~/tbp/results/monty/projects/evidence_eval_runs/logs/\")\n",
    "exp_name = \"base_config_10distinctobj_dist_agent\"\n",
    "exp_path = log_path + exp_name\n",
    "\n",
    "train_stats, eval_stats, detailed_stats, lm_models_base_objects = load_stats(exp_path,\n",
    "                                                                load_train=False, # doesn't load train csv\n",
    "                                                                load_eval=False, # loads eval_stats.csv\n",
    "                                                                load_detailed=False, # doesn't load .json\n",
    "                                                                load_models=True, # loads .pt models\n",
    "                                                                pretrained_dict=pretrained_dict,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b59db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_models['pretrained'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95517886",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_models['pretrained'][0]['021_logo_tbp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(lm_models['pretrained'][0]['022_logo_numenta']['patch_0'], rotation=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e46ad4",
   "metadata": {},
   "source": [
    "\n",
    "## Comparing Lower and Higher Level LM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lllm_hllm_models(lm_models, object_id, show_ax_ticks=False, elev=-80, azim=180, roll=180):\n",
    "    lllm_model = lm_models['pretrained'][0][object_id]['patch_0']\n",
    "    hllm_model = lm_models['pretrained'][1][object_id]['patch_1']\n",
    "\n",
    "    lllm_color_idxs = lllm_model.feature_mapping['hsv']\n",
    "    lllm_colors = lllm_model.x[:, lllm_color_idxs[0]:lllm_color_idxs[1]]\n",
    "    hllm_color_idxs = hllm_model.feature_mapping['hsv']\n",
    "    hllm_colors = hllm_model.x[:, hllm_color_idxs[0]:hllm_color_idxs[1]]\n",
    "\n",
    "    # Conver HSV values to RGB\n",
    "    lllm_rgb_colors = [colorsys.hsv_to_rgb(*hsv) for hsv in lllm_colors]\n",
    "    hllm_rgb_colors = [colorsys.hsv_to_rgb(*hsv) for hsv in hllm_colors]\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 2, 1, projection='3d')\n",
    "    ax1.scatter(lllm_model.pos[:, 1], lllm_model.pos[:, 0], lllm_model.pos[:, 2], c=lllm_rgb_colors)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(f\"LLLM model of \\n{object_id}\")\n",
    "    ax1.view_init(elev, azim, roll)\n",
    "    if not show_ax_ticks:\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_zticks([])\n",
    "    ax2 = plt.subplot(1, 2, 2, projection='3d')\n",
    "    ax2.scatter(hllm_model.pos[:, 1], hllm_model.pos[:, 0], hllm_model.pos[:, 2], c=hllm_rgb_colors)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(f\"HLLM model of \\n{object_id}\")\n",
    "    ax2.view_init(elev, azim, roll)\n",
    "    if not show_ax_ticks:\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_zticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1603b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_models['pretrained'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lllm_hllm_models(avg_hsv_models, '001_cube', elev=-50, azim=150, roll=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lllm_hllm_models(lm_models_base_objects, '001_cube', elev=-50, azim=150, roll=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lllm_hllm_models(lm_models_base_objects, '021_logo_tbp', elev=-80, azim=180, roll=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010c458",
   "metadata": {},
   "source": [
    "### Compositional Models Learned with Single Level of Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28233059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT_WITH_LOGOS = [\n",
    "#     \"002_cube_tbp_horz\",\n",
    "#     \"004_cube_numenta_horz\",\n",
    "#     \"007_disk_tbp_horz\",\n",
    "#     \"009_disk_numenta_horz\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8897cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model that has trained on the compositional objects, but with no hierarchical passing of model IDs\n",
    "pretrained_dict = pretrain_path + \"pretrained_ycb_v10/supervised_pre_training_compositional_objects_with_logos/pretrained/\"\n",
    "\n",
    "log_path = os.path.expanduser(\"~/tbp/results/monty/projects/evidence_eval_runs/logs/\")\n",
    "exp_name = \"base_config_10distinctobj_dist_agent\"\n",
    "exp_path = log_path + exp_name\n",
    "\n",
    "__, __, __, lm_models_single_level = load_stats(exp_path,\n",
    "                                                                load_train=False, # doesn't load train csv\n",
    "                                                                load_eval=False, # loads eval_stats.csv\n",
    "                                                                load_detailed=False, # doesn't load .json\n",
    "                                                                load_models=True, # loads .pt models\n",
    "                                                                pretrained_dict=pretrained_dict,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb19444",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lllm_hllm_models(lm_models_single_level, '009_disk_numenta_horz', elev=-50, azim=150, roll=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07236090",
   "metadata": {},
   "source": [
    "#### Smaller Step Size (1 rather than 5 degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd548f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model that has trained on the compositional objects, but with no hierarchical passing of model IDs\n",
    "pretrained_dict = pretrain_path + \"pretrained_ycb_v10/supervised_pre_training_compositional_objects_with_logos_small_step_size/pretrained/\"\n",
    "\n",
    "log_path = os.path.expanduser(\"~/tbp/results/monty/projects/evidence_eval_runs/logs/\")\n",
    "exp_name = \"base_config_10distinctobj_dist_agent\"\n",
    "exp_path = log_path + exp_name\n",
    "\n",
    "__, __, __, lm_models_single_level_small_step_size = load_stats(exp_path,\n",
    "                                                                load_train=False, # doesn't load train csv\n",
    "                                                                load_eval=False, # loads eval_stats.csv\n",
    "                                                                load_detailed=False, # doesn't load .json\n",
    "                                                                load_models=True, # loads .pt models\n",
    "                                                                pretrained_dict=pretrained_dict,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lllm_hllm_models(lm_models_single_level_small_step_size, '009_disk_numenta_horz', elev=-50, azim=150, roll=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5b858",
   "metadata": {},
   "source": [
    "## Partially Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fa533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_path = os.path.expanduser(\"~/tbp/results/monty/pretrained_models/\")\n",
    "pretrained_dict = pretrain_path + \"pretrained_ycb_v10/partial_supervised_pre_training_comp_objects/pretrained/\"\n",
    "\n",
    "log_path = os.path.expanduser(\"~/tbp/results/monty/projects/evidence_eval_runs/logs/\")\n",
    "exp_name = \"base_config_10distinctobj_dist_agent\"\n",
    "exp_path = log_path + exp_name\n",
    "\n",
    "train_stats, eval_stats, detailed_stats, lm_models_compositional = load_stats(exp_path,\n",
    "                                                                load_train=False, # doesn't load train csv\n",
    "                                                                load_eval=False, # loads eval_stats.csv\n",
    "                                                                load_detailed=False, # doesn't load .json\n",
    "                                                                load_models=True, # loads .pt models\n",
    "                                                                pretrained_dict=pretrained_dict,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_models_compositional['pretrained'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_models_compositional['pretrained'][1]['004_cube_numenta_horz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7101ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compositional_models(lm_models, object_id, show_ax_ticks=False, elev=-80, azim=180, roll=180):\n",
    "    patch_1_model = lm_models['pretrained'][1][object_id]['patch_1']\n",
    "    lm0_model = lm_models['pretrained'][1][object_id]['learning_module_0']\n",
    "\n",
    "    hllm_color_idxs = patch_1_model.feature_mapping['hsv']\n",
    "    hllm_colors = patch_1_model.x[:, hllm_color_idxs[0]:hllm_color_idxs[1]]\n",
    "    # Conver HSV values to RGB\n",
    "    hllm_rgb_colors = [colorsys.hsv_to_rgb(*hsv) for hsv in hllm_colors]\n",
    "\n",
    "    hllm_obj_idxs = lm0_model.feature_mapping['object_id']\n",
    "    hllm_obj_ids = lm0_model.x[:, hllm_obj_idxs[0]:hllm_obj_idxs[1]]\n",
    "    print(np.unique(hllm_obj_ids))\n",
    "\n",
    "    plt.figure()\n",
    "    ax1 = plt.subplot(1, 2, 1, projection='3d')\n",
    "    ax1.scatter(patch_1_model.pos[:, 1], patch_1_model.pos[:, 0], patch_1_model.pos[:, 2], c=hllm_rgb_colors)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_title(f\"LLLM model from SM input of \\n{object_id}\")\n",
    "    ax1.view_init(elev, azim, roll)\n",
    "    if not show_ax_ticks:\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_zticks([])\n",
    "    ax2 = plt.subplot(1, 2, 2, projection='3d')\n",
    "    ax2.scatter(lm0_model.pos[:, 1], lm0_model.pos[:, 0], lm0_model.pos[:, 2], c=hllm_obj_ids)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_title(f\"HLLM model from LM input \\n{object_id}\")\n",
    "    ax2.view_init(elev, azim, roll)\n",
    "    if not show_ax_ticks:\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_zticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215129fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compositional_models(lm_models_compositional, '007_disk_tbp_horz', elev=-50, azim=150, roll=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a1f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340e767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feat.compositional_testbed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
